
# coding: utf-8

# Lucas Bouchard
# 
# # NCAA Tournament Project: Web Scraping, graphing, and networking 
# 
# 

# In[1]:

get_ipython().magic('matplotlib inline')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#import seaborn as sb
import requests
import networkx as nx
from bs4 import BeautifulSoup, NavigableString


# # Getting the data

# Tournament page data

# In[2]:

raw = requests.get('http://www.sports-reference.com/cbb/postseason/2017-ncaa.html').text
soup = BeautifulSoup(raw, 'lxml')


# Parsing out all the divisions.

# In[3]:

east_soup = soup.find_all('div',{'id':'east'})
east_soup[0]
midwest_soup = soup.find_all('div',{'id':'midwest'})
south_soup = soup.find_all('div',{'id':'south'})
west_soup = soup.find_all('div',{'id':'west'})


# In[4]:

east_soup[0].find_all('a')[5].text


# In[5]:

teams_href_list = list()
for link in east_soup[0].find_all('a'):
    if len(link.text) > 0:
        teams_href_list.append(link['href'])

#teams_href_list        


# # Parsing the HTML to get all the teams in each division
# 

# In[6]:

def get_teams(division_soup):
    teams_href_list = list()
    for link in division_soup[0].find_all('a'):
        if len(link.text) > 0 and 'schools' in link['href']:
            teams_href_list.append(link)
            
    return teams_href_list


# # Using 'soup' to put each division in list to see all teams in 2017 tournament. 
# 
# 

# In[7]:

east_teams_2017 = get_teams(east_soup)
midwest_teams_2017 = get_teams(midwest_soup)
south_teams_2017 = get_teams(south_soup)
west_teams_2017 = get_teams(west_soup)
teams_2017 = east_teams_2017 + midwest_teams_2017 + south_teams_2017 + west_teams_2017
len(teams_2017)






# In[8]:

cleaned_teams = list()
for team in teams_2017:
    if team not in cleaned_teams:
        cleaned_teams.append(team)
len(cleaned_teams)        


# # Looping through 'teams_2017' to make 'tournament_teams' by applying the '.text' to each element which extracts the cleaned name of the team and appends it to new list. 
# 
# 

# In[9]:

tournament_teams = list()
for team in teams_2017:
    if team.text not in tournament_teams:
        tournament_teams.append(team.text)
#tournament_teams        
        
        
        


# # Getting the `href` out of a single team in `teams_2017`.

# In[10]:

teams_2017[0]['href']


# # Using the `replace` string function to update the `href' so we can get 2017-schedual

# In[11]:

'http://www.sports-reference.com' + teams_2017[0]['href'].replace('2017','2017-schedule')


# # Making a `_url` variable that combines the domain ("http://www.sports-reference.com/") and the `href` updated to contain "2017-schedule.html". Then used requests' `get` and `text` methods to get the raw HTML, saving it as `team_raw.` Madee `team_soup` out of `team_raw,` and then found specific table containing the schedule 
# 
# 

# In[12]:

raw = requests.get('http://www.sports-reference.com/cbb/schools/virginia/2017-schedule.html').text
soup = BeautifulSoup(raw, 'lxml')

schedule_table = soup.find_all('table',{'id':'schedule','class':"sortable stats_table"})
row = schedule_table[0].find_all('tr')[1]
row
#.find_all('td')


# In[13]:

schedule_table[0].find_all('td',{'data-stat':'date_game'})[0].text


# # Parsing the `schedule_table`
# 
# 

# In[14]:

schedule_table[0].find_all('td',{'data-stat':'opp_name'})


# # Parsing out the opponents and saving list of values as `opponents`.

# In[15]:

opponents = list()
for opp in schedule_table[0].find_all('td',{'data-stat':'opp_name'}):
    opponents.append(opp.text)
opponents    


# # Parsing out the results and saving list of values as `results`.

# In[16]:

results = list()
for sult in schedule_table[0].find_all('td',{'data-stat':'game_result'}):
    results.append(sult.text)
#results    


# # Parsing out the team score and saving list of values as `team_scores`.

# In[17]:

team_score = list()
for point in schedule_table[0].find_all('td',{'data-stat':'pts'}):
    team_score.append(point.text)
#team_score    


# # Parsing out the opponent's score and saving list of values as `opp_scores`.

# In[18]:

opp_score = list()
for point in schedule_table[0].find_all('td',{'data-stat':'opp_pts'}):
    opp_score.append(point.text)
#opp_score    


# In[19]:

list1 = ['a','b','c']
list2 = [0,1,2]
list3 = ['alpha','beta','gamma']

zip(list1,list2,list3)


# In[20]:

list(zip(list1,list2,list3))


# In[21]:

list1 = opponents
list2 = results
list3 = team_score
list4 = opp_score

team_results=list(zip(list1,list2,list3,list4))
team_results


# # Crawling all of the teams' tables

# Here I create a loop that goes through each team in `teams_2017` and does all of the steps above in each pass.
# 
# 0. Before the loop starts, has an empty list `all_results`
# 1. Checks to make sure the `team.text` isn't empty, skips urls that point to "tbd", or handles index position errors from pages with empty tables.
# 2. Takes the team object the url with the `href` replacement
# 3. Uses requests' `get` and `text` methods to get the raw HTML
# 4. Turns the raw HTML into Soup
# 5. Finds the table
# 6. Finds the dates, opponents, results, team score, and opponent scores
# 7. Zips them all together into `team_results`
# 8. For each result in `team_results` appends it to `all_results`
# 

# In[22]:

len(teams_2017)
cleaned_teams[0]['href'].replace('2017.html' , '2017_schedule.html')


# In[23]:

all_teams = list()
for team in cleaned_teams:
    url = 'http://www.sports-reference.com' + team['href'].replace('2017.html','2017-schedule.html')
    raw = requests.get(url).text
    soup = BeautifulSoup(raw,'lxml')
    schedule_table = soup.find_all('table',{'id':'schedule','class':'sortable stats_table'})
    all_results = list()
    all_results1 = list()
    result = list()
    all_results3 = list()
    all_results4 = list()
    for opp in schedule_table[0].find_all('td',{'data-stat':'date_game'}):
        all_results.append(opp.text)
    for opp in schedule_table[0].find_all('td',{'data-stat':'opp_name'}):
        all_results1.append(opp.text)
    for opp in schedule_table[0].find_all('td',{'data-stat':'game_result'}):
        result.append(opp.text)   
    for opp in schedule_table[0].find_all('td',{'data-stat':'pts'}):
        all_results3.append(opp.text)     
    for opp in schedule_table[0].find_all('td',{'data-stat':'opp_pts'}):
        all_results4.append(opp.text)      
    
    home_team = [team.text] * len(result)
    team_result = list(zip(home_team, all_results, all_results1, result, all_results3, all_results4))
    all_teams.append(team_result)
all_teams    
        


# In[ ]:

reduced_results = list()
for team in all_teams:
    for result in team:
        if 'W' in result:
            reduced_results.append(result)
            

        
len(all_teams), len(reduced_results)


# In[ ]:

tournament_results = list()
for result in reduced_results:
    if result[2] in tournament_teams:
        tournament_results.append(result)
tournament_results        

[('Northwestern', 'Dayton', '2016-12-17', 'W', '67', '64'),
 ('Northwestern', 'Wisconsin', '2017-02-12', 'W', '66', '59'),
 ('Northwestern', 'Michigan', '2017-03-01', 'W', '67', '65'),
 ('Northwestern', 'Maryland', '2017-03-10', 'W', '72', '64')]
# # Making a directed graph
# 
# 

# In[ ]:

g = nx.DiGraph()

for (team, opponent, date, score, opp_score) in tournament_results:
    differential = int(score) - int(opp_score)
    if differential > 0:
        if g.has_edge(team,opponent):
            g[team][opponent]['weight'] += differential
        else:
            g.add_edge(team, opponent, weight = abs(differential))
    else:
        print('Negative point differential for: {0}-{1}\n'.format(team,opponent))
        if g.has_edge(team,opponent):
            g[team][opponent]['weight'] += differential
        else:
            g.add_edge(team, opponent, weight = abs(differential))
        
print("There are {0} nodes and {1} edges in the network".format(g.number_of_nodes(), g.number_of_edges()))

nx.write_gexf(g,'tournament_schedule.gexf')


# In[ ]:

import networkx as nx


# In[ ]:

g = nx.Graph()


# In[ ]:

g.add_node('a')


# In[ ]:

len(g)


# In[ ]:

g.nodes()


# In[ ]:

g.add_nodes_from(['b','c','d'])


# In[ ]:

g.nodes()


# In[ ]:

len(g)


# In[ ]:

g.remove_edge('a','b')


# In[ ]:

g.edges()


# In[ ]:

g.add_edges_from([('a','b'),('a','c'),('a','d')])


# In[ ]:

g.edges()


# In[ ]:

g.add_edge('d','a')
g.edges()


# In[ ]:

g.edges()
np.array(nx)


# # Making a DiGraph

# In[ ]:

dg = nx.DiGraph()


# In[ ]:

dg.add_edges_from([('a','b'),('a','c'),('a','d')])


# In[ ]:

dg.nodes()


# In[ ]:

dg.edges()


# In[ ]:

dg.add_edge('c','a')


# In[ ]:

dg.edges()

